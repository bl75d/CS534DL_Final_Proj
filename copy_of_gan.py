# -*- coding: utf-8 -*-
"""Copy of GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iehLLOFYLUXi6XXjbWSt6Yb4iFWXRMRP

# full code
"""

import numpy as np
import torch
import torch.nn
from torch import optim
import torch.nn.functional as F
import matplotlib.pylab as plt
import numpy as np

SAMPlE_DIMS = 100 # dimension of random noise
CHANNELS = 10 # num of channels to be used in 2D convolutions
IMG_FC= 784 # fully connected img size(input img size)
HIDDEN = 512 # size of hidden fully connected layers
BATCH_SIZE = 100 # mini batch size
LEARING_RATE_D = 1e-4 # learning rate of Didcriminator
LEARING_RATE_G = 1e-3 # learning rate of generator
N_BATCHES_D = 200 # number of minibatches to train D
N_BATCHES_G = 500 # number of minibatches to train G
K = 1 # training epochs for D 
I = 1 # training epochs for G
NUM_ITERATIONS =200 # number of overall training iterations

class Generator (torch.nn.Module):
    '''take a Gaussian noise vector as input and 
       produce a vector in [0, 1] 784 as output.'''
    def __init__ (self):
        super(Generator, self).__init__()
        c = CHANNELS
        self.fc = torch.nn.Linear(in_features=100, out_features=c*2*7*7)
        self.bn1 = torch.nn.BatchNorm1d(c*2*7*7) 
        self.conv1 = torch.nn.ConvTranspose2d(in_channels=c*2, out_channels=c, kernel_size=4, stride=2, padding=1)
        self.bn2 = torch.nn.BatchNorm2d(c) 
        self.conv2 = torch.nn.ConvTranspose2d(in_channels=c, out_channels=1, kernel_size=4, stride=2, padding=1)

    def forward (self, x):
        x = torch.relu(self.fc(x))
        x = self.bn1(x)
        x = x.view(x.size(0), CHANNELS*2, 7, 7)
        x = F.relu(self.conv1(x))
        x = self.bn2(x)
        x = torch.sigmoid(self.conv2(x))
        x = x.view(BATCH_SIZE,IMG_FC)
        return x

class Discriminator (torch.nn.Module):
    '''take a vector in R784 as input and 
       produce the probability yˆ ∈ [0, 1] of the image being real.'''
    def __init__ (self):
        super(Discriminator, self).__init__()
        c = CHANNELS
        self.main = torch.nn.Sequential(
              torch.nn.Conv2d(1, c, 4, 2, 1, bias=False),
              torch.nn.Dropout2d(0.2),
              torch.nn.LeakyReLU(0.2, inplace=True), # c x 14 x 14
              torch.nn.Conv2d(c, c * 2, 4, 2, 1, bias=False),
              torch.nn.BatchNorm2d(c* 2),
              torch.nn.LeakyReLU(0.2, inplace=True),  #(c*2) x 7 x 7
              torch.nn.Flatten(),
              torch.nn.Linear(in_features=c*2*7*7, out_features=1),
              torch.nn.Sigmoid()
            )

    def forward(self, x):
        x = x.view(-1,1,28,28)
        x = self.main(x)
        return x 
    

def show_results(loss_d_iter, loss_g_iter, acc_d_iter, acc_g_iter):
    plt.plot(loss_d_iter)
    plt.plot(loss_g_iter)
    plt.legend(['Discriminator','Generator'],prop={'size': 13})
    plt.title("Loss")
    plt.show()

    plt.plot(acc_d_iter)
    plt.plot(acc_g_iter)
    plt.legend(['Discriminator','Generator'],prop={'size': 13})
    plt.title("Accuracy")
    plt.show()

def train (discriminator ,generator, X):
    criterion = torch.nn.BCELoss()
    optimizerG = optim.Adam(generator.parameters(), lr=LEARING_RATE_G) 
    optimizerD = optim.Adam(discriminator.parameters(), lr=LEARING_RATE_D)
    loss_d_iter = []
    loss_g_iter = []
    acc_d_iter = []
    acc_g_iter = []
    N_BATCHES_D1 = N_BATCHES_D
    for it in range(NUM_ITERATIONS): 
        loss_d = 0
        correct_d = 0
        n_data_d = 0
        for k in range(K):   
            for i in np.arange(0, len(X[:N_BATCHES_D1*BATCH_SIZE]), BATCH_SIZE):
                miniBatch = X[i:i+BATCH_SIZE]
                samples = torch.normal(0.0, 1.0, 
                                      (BATCH_SIZE, SAMPlE_DIMS)).to(device) 
                generator.eval() 
                noise = generator(samples)
                inputs = torch.cat((miniBatch, noise) , dim = 0)
                targets = torch.cat((torch.ones(BATCH_SIZE,1), 
                                    torch.zeros(BATCH_SIZE,1)), dim = 0)
                shuffle = torch.randperm(inputs.size()[0])
                inputs = inputs[shuffle].to(device)
                targets = targets[shuffle].to(device)

                discriminator.train()
                outputs = discriminator(inputs)
                loss = criterion(outputs, targets)
                loss_d += loss.item()*BATCH_SIZE*2
                n_data_d += BATCH_SIZE*2
                predicted = 1*(outputs>=0.5)
                correct_d += (predicted == targets).sum().item()
                
                optimizerD.zero_grad()
                loss.backward()
                optimizerD.step()

        loss_g = 0
        n_data_g= 0
        correct_g = 0
        for i in range(I):  
            for i in np.arange(0, len(X[:N_BATCHES_G*BATCH_SIZE]), BATCH_SIZE): # mini-batches
                samples = torch.normal(0.0, 1.0, 
                                      (BATCH_SIZE, SAMPlE_DIMS)).to(device)
                generator.train()
                noise = generator(samples).to(device)
                targets = torch.ones(BATCH_SIZE,1).to(device)

                discriminator.eval() 
                outputs = discriminator(noise)
                loss = criterion(outputs, targets)
                loss_g += loss.item()*BATCH_SIZE
                n_data_g += BATCH_SIZE
                predicted = 1*(outputs>=0.5)
                correct_g += (predicted == targets).sum().item()

                optimizerG.zero_grad()
                loss.backward()
                optimizerG.step()

        lossD = loss_d/n_data_d
        accD = correct_d/n_data_d
        loss_d_iter.append(lossD)
        acc_d_iter.append(accD)

        lossG = loss_g/n_data_g
        accG = correct_g/n_data_g
        loss_g_iter.append(lossG)
        acc_g_iter.append(accG)
        pstring = "iteration {}/{}, D loss: {:.2f}, G loss: {:.2f}, D acc: {:.2f}%, G acc: {:.2f}%"
        print(pstring.format(it+1, NUM_ITERATIONS, lossD, lossG, accD*100, accG*100))
        
        if lossG < lossD -0.1:
            if (N_BATCHES_D1 + 100) < len(X)/BATCH_SIZE:
                N_BATCHES_D1  +=100
        elif lossG < lossD-0.05:
            if (N_BATCHES_D1 + 50) < len(X)/BATCH_SIZE:
                N_BATCHES_D1  +=50

    show_results(loss_d_iter, loss_g_iter, acc_d_iter, acc_g_iter)
    plt.figure(figsize=(15,2))
    for i in range(20):
        plt.subplot(1,20, i+1)
        plt.imshow(noise[i].view(28, 28).cpu().detach(),cmap='gray')
        plt.axis('off')
    plt.tight_layout(pad=0)
    plt.show()

    return discriminator ,generator, loss_d_iter, loss_g_iter, acc_d_iter, acc_g_iter

if __name__ == "__main__":
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")    
    X = torch.from_numpy(np.load("mnist.npy")).float().to(device)
    X = X[torch.randperm(X.size()[0])]
    discriminator = Discriminator().to(device)
    generator = Generator().to(device)
    discriminator ,generator, *_ = train (discriminator,generator, X)

"""# experiments on hyperparameters"""

LEARING_RATE_D = 1e-4
LEARING_RATE_G = 1e-3
N_BATCHES_D = 200
N_BATCHES_G = 500
NUM_ITERATIONS =60
discriminator = Discriminator().to(device)
generator = Generator().to(device)
discriminator ,generator, loss_d_iter, loss_g_iter, acc_d_iter, acc_g_iter = train (discriminator,generator, X)

LEARING_RATE_D = 1e-4
LEARING_RATE_G = 1e-3
N_BATCHES_D = 200
N_BATCHES_G = 500
NUM_ITERATIONS =200
discriminator = Discriminator().to(device)
generator = Generator().to(device)
discriminator ,generator, loss_d_iter, loss_g_iter, acc_d_iter, acc_g_iter = train (discriminator,generator, X)

LEARING_RATE_D = 1e-4
LEARING_RATE_G = 1e-3
N_BATCHES_D = 200
N_BATCHES_G = 500
NUM_ITERATIONS =100
discriminator = Discriminator().to(device)
generator = Generator().to(device)
discriminator ,generator, loss_d_iter, loss_g_iter, acc_d_iter, acc_g_iter = train (discriminator,generator, X)

samples = torch.normal(0.0, 1.0, (BATCH_SIZE,SAMPlE_DIMS)).to(device) 
generator.eval() 
noise = generator(samples)

plt.figure(figsize=(15,2))
for i in range(20):
    plt.subplot(1,20, i+1)
    plt.imshow(noise[i].view(28, 28).cpu().detach(),cmap='gray')
    plt.axis('off')
plt.tight_layout(pad=0)
plt.show()

LEARING_RATE_D = 1e-3
LEARING_RATE_G = 1e-3
N_BATCHES_D = 200
N_BATCHES_G = 500
NUM_ITERATIONS =30
discriminator = Discriminator().to(device)
generator = Generator().to(device)
discriminator ,generator, loss_d_iter, loss_g_iter, acc_d_iter, acc_g_iter = train (discriminator,generator, X)

LEARING_RATE_D = 1e-3
LEARING_RATE_G = 1e-3
N_BATCHES_D = 100
N_BATCHES_G = 500
NUM_ITERATIONS =60
discriminator = Discriminator().to(device)
generator = Generator().to(device)
discriminator ,generator, loss_d_iter, loss_g_iter, acc_d_iter, acc_g_iter = train (discriminator,generator, X)

LEARING_RATE_D = 1e-3
LEARING_RATE_G = 1e-3
N_BATCHES_D = 100
N_BATCHES_G = 500
NUM_ITERATIONS =200
discriminator = Discriminator().to(device)
generator = Generator().to(device)
discriminator ,generator, loss_d_iter, loss_g_iter, acc_d_iter, acc_g_iter = train (discriminator,generator, X)

LEARING_RATE_D = 1e-3 # learning rate of Didcriminator
LEARING_RATE_G = 1e-3 # learning rate of generatot
N_BATCHES_D = 200 # number of minibatches to train D
N_BATCHES_G = 500 # number of minibatches to train G
NUM_ITERATIONS = 50
dis = Discriminator().to(device)
gen = Generator().to(device)
dis ,gen, *_ = train (dis,gen, X)

for i, param in enumerate(dis.parameters()): 
    if i < 4:
        param.requires_grad = False
    else: 
        param.requires_grad = True
for i, param in enumerate(gen.parameters()): 
    if i < 8:
        param.requires_grad = False
    else: 
        param.requires_grad = True

import copy

gen1 = copy.deepcopy(gen)
dis1 = copy.deepcopy(dis)

gen1 = copy.deepcopy(gen)
dis1 = copy.deepcopy(dis)
LEARING_RATE_D = 1e-4 # learning rate of Didcriminator
LEARING_RATE_G = 1e-3 # learning rate of generatot
N_BATCHES_D = 300 # number of minibatches to train D
N_BATCHES_G = 500 # number of minibatches to train G
NUM_ITERATIONS = 20
dis1 ,gen1, *_ = train (dis1, gen1, X)

LEARING_RATE_D = 1e-4 # learning rate of Didcriminator
LEARING_RATE_G = 1e-3 # learning rate of generatot
N_BATCHES_D = 300 # number of minibatches to train D
N_BATCHES_G = 500 # number of minibatches to train G
NUM_ITERATIONS = 20

for i, param in enumerate(dis.parameters()): 
    if i < 4:
        param.requires_grad = False
    else: 
        param.requires_grad = True

for i, param in enumerate(gen1.parameters()): 
    if i < 6:
        param.requires_grad = False
    else: 
        param.requires_grad = True
dis1 ,gen1, *_ = train (dis1, gen1, X)

