# -*- coding: utf-8 -*-
"""Copy of Stock_GAN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SsIHvJCxpaUJ9fjcX7AWd783C15oJQEu

# 1. Get data
"""

import os
import sys
from google.colab import drive
drive.mount('/content/gdrive')
path0 = F"/content/gdrive/MyDrive/Deep_Learning_HW/FinalProject/CS534DL_Final_Proj-main"
module_path = os.path.abspath(os.path.join(path0))
if module_path not in sys.path:
    sys.path.append(module_path)
module_path

!pip install tslearn
!pip install yfinance 
!pip install stockstats
!pip install stock-pandas

from DataPreprocessing.Processing2 import SplitData,DataPipeline, GetclosePrice
import numpy as np

# Specify the stocks you want.
symbol_list = ['AAPL','TSLA','AMZN','GOOG','BLNK','PLTR','SNAP']
period = "1y"
interval = "1d"
#size of databrick(eg.49 days/hours as a databrick for training)
size=49
# return a dictionary of stocks, and a dictionary of labels----{stock_symbol:stock_data;label symbol:label_data}
StockDict, LabelDict=DataPipeline(symbol_list,period,interval)

# get price list for testing set to calculate NAV
StockPriceDic=GetclosePrice(symbol_list,period,interval,size)

# Generate Xtrain, Xtest, ytrain, ytest for training and testing
Xtrain, Xtest, ytrain, ytest = SplitData(StockDict[symbol_list[0]], LabelDict[symbol_list[0]], size)
for i in range(1,len(symbol_list)):
    X_train, X_test, y_train, y_test=SplitData(StockDict[symbol_list[i]],LabelDict[symbol_list[i]],size)
    Xtrain=np.concatenate((Xtrain,X_train),axis=0)
    Xtest = np.concatenate((Xtest, X_test), axis=0)
    ytrain = np.concatenate((ytrain, y_train), axis=0)
    ytest = np.concatenate((ytest, y_test), axis=0)

plt.plot(StockPriceDic[symbol_list[0]])

StockPriceDic[symbol_list[0]][1],ytrain[0]

plt.plot(ytrain)

Xtrain.shape, Xtest.shape, ytrain.shape, ytest.shape

Xtrain.max(), Xtrain.min()

# make training and test sets in torch
# x_train = torch.from_numpy(Xtrain).type(torch.Tensor)
# x_test = torch.from_numpy(Xtest).type(torch.Tensor)
# y_train = torch.from_numpy(ytrain).type(torch.Tensor)
# y_test = torch.from_numpy(ytest).type(torch.Tensor)
# torch.save(x_train, path0+'x_train.pt')
# torch.save(x_test, path0+'x_test.pt')
# torch.save(y_train, path0+'y_train.pt') 
# torch.save(y_test, path0+'y_test.pt')

"""# 2. Model"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount('/content/gdrive')
path0 = F"/content/gdrive/MyDrive/Deep_Learning_HW/FinalProject/CS534DL_Final_Proj-main"

x_train =  torch.load(path0+'x_train.pt')
x_test = torch.load(path0+'x_test.pt')
y_train = torch.load(path0+'y_train.pt')
y_test = torch.load(path0+'y_test.pt')

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler(feature_range=(-1, 1))
y_train_sc = scaler.fit_transform(y_train)
y_train_sc = torch.from_numpy(y_train_sc).type(torch.Tensor)

x_train.size(), y_train_sc.size()

# Build model
#####################
input_dim = 47
hidden_dim = 100
num_layers = 3
output_dim = 1

# Here we define our model as a class
class LSTM(nn.Module):
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):
        super(LSTM, self).__init__()
        # Hidden dimensions
        self.hidden_dim = hidden_dim

        # Number of hidden layers
        self.num_layers = num_layers
        # batch_first=True causes input/output tensors to be of shape
        # (batch_dim, seq_dim, feature_dim)
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        # Readout layer
        self.fc1 = nn.Linear(hidden_dim, output_dim)
  

    def forward(self, x):
        # print("x shape:",x.shape)
        # Initialize hidden state with zeros
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()
        # Initialize cell state
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).requires_grad_()
        # print("h0",h0.shape)
        # We need to detach as we are doing truncated backpropagation through time (BPTT)
        # If we don't, we'll backprop all the way to the start even after going through another batch
        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))
        # print("lstm out:",out.shape)
        # print("out[:, -1, :]:",out[:, -1, :].shape)
        # Index hidden state of last time step
        # out[:, -1, :] --> just want last time step hidden states! 
        # or else will return 49 days of hidden states
        out = self.fc1(out[:, -1, :])
        # print("final out:",out.shape)
        return out
    
model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)

loss_fn = torch.nn.MSELoss()
print(model)
print(len(list(model.parameters())))
for i in range(len(list(model.parameters()))):
    print(list(model.parameters())[i].size())

y_train_pred = model(x_train)
y_train_pred.shape

import copy

# Train model
num_epochs = 200
# Number of steps to unroll
seq_dim =49
model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)
loss_fn = torch.nn.MSELoss()
optimiser = torch.optim.Adam(model.parameters(), lr=0.001)

hist = np.zeros(num_epochs)
hist_val = np.zeros(num_epochs)
best_val = 1000
for t in range(num_epochs):
    # Initialise hidden state
    # Don't do this if you want your LSTM to be stateful
    # model.hidden = model.init_hidden()
    
    # Forward pass
    y_train_pred = model(x_train[:-200])

    loss = loss_fn(y_train_pred, y_train_sc[:-200])
    # Zero out gradient, else they will accumulate between epochs
    optimiser.zero_grad()
    # Backward pass
    loss.backward()
    # Update parameters
    optimiser.step()
    # if t % 10 == 0 and t !=0:
    model.eval()
    y_val_pred = model(x_train[-200:])
    loss_val = loss_fn(y_val_pred, y_train_sc[-200:])
    print("Epoch", t+1, "Training MSE: {:.4f}".format(loss.item()),
          "Val MSE: {:.4f}".format(loss_val.item()))
    # print()
    hist[t] = loss.item()
    hist_val[t] = loss_val.item()
    if loss_val < best_val:
      best_val = loss_val
      best_model = copy.deepcopy(model)

plt.plot(hist)
plt.plot(hist_val)

best_model.eval()
y_train_pred = best_model(x_train[:-200])
y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())
y_val_pred = best_model(x_train[-200:])
y_val_pred = scaler.inverse_transform(y_val_pred.detach().numpy())
y_pred = np.concatenate((y_train_pred, y_val_pred),axis = 0)
print("true loss:",loss_fn(torch.Tensor(y_pred), torch.Tensor(ytrain)).item())

# Visualising the results
figure, axes = plt.subplots(figsize=(15, 6))
axes.plot(y_train.detach().numpy(), color = 'blue', label = 'Real Stock Price')
axes.plot(y_pred, color = 'red', label = 'Predicted Stock Price')
#axes.xticks(np.arange(0,394,50))
plt.axvline(len(y_tr)-200, 0, 3500, label='validation line')
plt.title('All Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.legend()
plt.show()

# APPL
figure, axes = plt.subplots(figsize=(15, 6))
axes.plot(y_train[:182].detach().numpy(), color = 'blue', label = 'Real Stock Price')
axes.plot(y_pred[:182], color = 'red', label = 'Predicted Stock Price')
#axes.xticks(np.arange(0,394,50))
plt.title('AAPL Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.legend()
plt.show()

"""# APPL stock"""

# Train model
num_epochs = 100
# Number of steps to unroll
seq_dim =49
num_layers = 4
model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, num_layers=num_layers)
loss_fn = torch.nn.MSELoss()
optimiser = torch.optim.Adam(model.parameters(), lr=0.001)

hist = np.zeros(num_epochs)
hist_val = np.zeros(num_epochs)
best_val = 1000
for t in range(num_epochs):
    # Initialise hidden state
    # Don't do this if you want your LSTM to be stateful
    # model.hidden = model.init_hidden()
    
    # Forward pass
    y_train_pred = model(x_train[:150])

    loss = loss_fn(y_train_pred, y_train_sc[:150])
    # Zero out gradient, else they will accumulate between epochs
    optimiser.zero_grad()
    # Backward pass
    loss.backward()
    # Update parameters
    optimiser.step()
    # if t % 10 == 0 and t !=0:
    model.eval()
    y_val_pred = model(x_train[150:182])
    loss_val = loss_fn(y_val_pred, y_train_sc[150:182])
    print("Epoch", t+1, "Training MSE: {:.4f}".format(loss.item()),
          "Val MSE: {:.4f}".format(loss_val.item()))
    # print()
    hist[t] = loss.item()
    hist_val[t] = loss_val.item()
    if loss_val < best_val:
      best_val = loss_val
      best_model = copy.deepcopy(model)

best_model.eval()
y_train_pred = best_model(x_train[:150])
y_train_pred = scaler.inverse_transform(y_train_pred.detach().numpy())
y_val_pred = best_model(x_train[150:182])
y_val_pred = scaler.inverse_transform(y_val_pred.detach().numpy())
y_pred = np.concatenate((y_train_pred, y_val_pred),axis = 0)
print("true loss:",loss_fn(torch.Tensor(y_pred), torch.Tensor(ytrain[:182])).item())

# Visualising the results
figure, axes = plt.subplots(figsize=(15, 6))
axes.plot(y_train[:182].detach().numpy(), color = 'blue', label = 'Real Stock Price')
axes.plot(y_pred, color = 'red', label = 'Predicted Stock Price')
plt.axvline(150, 0, max(ytrain[:182]+100), label='validation line')
plt.title('Apple Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('Stock Price')
plt.legend()
plt.show()

